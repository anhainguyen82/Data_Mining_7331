{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Lab Two Classification</center>\n",
    "<center><font size = \"4\"> 2017-2018 California Department of Education Mathmematics Achievement</font></center>\n",
    "\n",
    "##### <center>Create by An Nguyen, Andy Ho, Jodi Pafford</center>\n",
    "<center> March 8, 2019</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation 1\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##Load original dataset.  No longer needed.\n",
    "##The cleaning process is very computationally expensive therefore a the 'ranifall.csv' file was created for later use. \n",
    "#rainfall_original = pd.read_csv('weatherAus.csv') \n",
    "\n",
    "#Functions to find the average value using the bracketing values around the NaN's.  \n",
    "    #For instance if a city's 'MinTemp' has 34, 32, NaN, NaN, 55 recorded \n",
    "    #the function will average 32 and 55 for the first NaN: (32+55)/2 = 43.5 \n",
    "    #and average the above value and 55 for the second NaN: (43.5+55)/2 = 49.25\n",
    "#Will only use values if they are from the same city.\n",
    "#If NaN is the earliest timepoint for a given city the next timepoint with no NaN will be given instead of the mean.\n",
    "#If NaN is the latest timepoint for a given city the previous timepoint with no NaN will be given instead of the mean.\n",
    "\n",
    "def impute_by_city(cities,variables):\n",
    "    for c in cities:\n",
    "        #aPrse out observations from a single city.\n",
    "        temp = rainfall[rainfall.Location == c]\n",
    "        \n",
    "        #Interate through all observations of the temp data file.\n",
    "        i = min(temp.index)\n",
    "        while i <= max(temp.index):\n",
    "            for v in variables:\n",
    "                #Check to see if there are values recorded for the variable, will pass over if all are NaN.\n",
    "                if pd.isna(temp[v]).all():\n",
    "                    pass\n",
    "                \n",
    "                #Check to see if a single value is NaN.\n",
    "                elif pd.isna(temp[v][i]):\n",
    "                    #Find the mean of bracketing values and impute into main dataframe.\n",
    "                    temp[v][i] = find_mean(temp[v], i)\n",
    "                    rainfall[v][i] = temp[v][i]\n",
    "            i = i + 1       \n",
    "\n",
    "#Find mean of bracketing values.\n",
    "def find_mean(templist, index):\n",
    "    #If NaN is earliest timepoint for the city take the next value that is not NaN.\n",
    "    if index == min(templist.index): \n",
    "        return find_top(templist, index)\n",
    "    \n",
    "    #If latest timepoint for the city take the previous value that is not NaN.\n",
    "    elif index == max(templist.index): \n",
    "        return find_bottom(templist, index)\n",
    "    \n",
    "    else:\n",
    "        #Find previous non-NaN value.\n",
    "        bottom = find_bottom(templist, index) \n",
    "        #Find next non-NaN value.\n",
    "        top = find_top(templist, index) \n",
    "        \n",
    "    #If current value is not from the latest timepoint for the city but there are no more non-NaN value recorded\n",
    "    #after this value then the previous non-NaN value will be taken.\n",
    "    if pd.isna(top): \n",
    "        return bottom\n",
    "    \n",
    "\n",
    "    else:\n",
    "        mean = (top + bottom)/2\n",
    "        return mean\n",
    "\n",
    "#Find previous non-NaN value.\n",
    "def find_bottom(templist, index):\n",
    "    while pd.isna(templist[index-1]):\n",
    "        index = index-1\n",
    "    bottom = templist[index-1]\n",
    "    return bottom\n",
    "\n",
    "#Find next non-NaN value.\n",
    "#If there are no more non-NaN values return the previous non-NaN value.\n",
    "def find_top(templist, index):\n",
    "    while pd.isna(templist[index+1]):\n",
    "        index = index+1\n",
    "        if index == max(templist.index):\n",
    "            top = np.nan\n",
    "            return top\n",
    "    top = templist[index+1]\n",
    "    return top   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code for first run data cleaning, no longer needed after 'rainfall.csv' was created at the end of cleaning process.\n",
    "\n",
    "#rainfall = rainfall_original.copy()\n",
    "\n",
    "##'RISK_MM' was used by creator of dataset to extrapolate response variable, 'RainTomorrow.'  Needs to be dropped to not \n",
    "##influence prediction.\n",
    "#rainfall.drop([\"RISK_MM\"], axis=1, inplace=True)\n",
    "\n",
    "##Drop any observation with no record of rainfall for the day.  Cannot be imputed.\n",
    "#rainfall.dropna(subset=[\"RainToday\"], inplace=True)\n",
    "\n",
    "#Reset the Index of each observation to match it's iloc, get rid of gaps between Index integers.\n",
    "#rainfall = rainfall.reset_index(drop=True)\n",
    "#rainfall.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##can be skipped if rainfall.csv already generated!\n",
    "\n",
    "##set the cardinal directions to degrees.\n",
    "#directions = {'N':0, 'NNE':22.5, 'NE':45, 'NE':45, 'ENE':67.5, 'E':90, 'ESE':112.5, 'SE':135, 'SSE':157.5, 'S':180,\\\n",
    "#              'SSW':202.5, 'SW':225, 'WSW':247.5, 'W':270, 'WNW':292.5, 'NW':315, 'NNW':337.5}\n",
    "\n",
    "##Replace cardianl direction to their corresponding degrees.\n",
    "#rainfall = rainfall.replace(directions) \n",
    "\n",
    "#Get name of all cities in the data frame.\n",
    "#cities = rainfall.Location.unique() \n",
    "\n",
    "#c_variables = []\n",
    "#d_variables = []\n",
    "\n",
    "##change 'Yes' and 'No' to 1 and 0 respectively.\n",
    "#rainfall.RainToday = rainfall.RainToday=='Yes'\n",
    "#rainfall.RainToday = rainfall.RainToday.astype(np.int)\n",
    "#rainfall.RainTomorrow = rainfall.RainTomorrow=='Yes'\n",
    "#rainfall.RainTomorrow = rainfall.RainTomorrow.astype(np.int)\n",
    "\n",
    "##Find all variables with continous data type.\n",
    "#for l in list(rainfall):\n",
    "#    if (rainfall[l].dtypes == 'float64'):\n",
    "#        c_variables.append(l)\n",
    "#    else:\n",
    "#        d_variables.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##can be skipped if rainfall.csv already generated! Very expensive, 'rainfall.csv' can be uploaded from working directory\n",
    "\n",
    "##Impute values to NaN's and save to csv file for later use.\n",
    "#impute_by_city(cities, c_variables)\n",
    "#rainfall.to_csv(\"rainfall.csv\", sep=',', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BadgerysCreek\n",
      "Newcastle\n",
      "NorahHead\n",
      "Penrith\n",
      "Tuggeranong\n",
      "MountGinini\n",
      "Nhil\n",
      "Dartmoor\n",
      "GoldCoast\n",
      "Adelaide\n",
      "Albany\n",
      "Witchcliffe\n",
      "SalmonGums\n",
      "Walpole\n"
     ]
    }
   ],
   "source": [
    "##load pre-generated rainfall.csv file.\n",
    "rainfall = pd.read_csv('rainfall.csv', index_col=0) \n",
    "\n",
    "#Variables 'Evaporation' and 'Sunshine' contained many missing values, too many to be imputed.\n",
    "rainfall = rainfall.drop(['Evaporation', 'Sunshine'], axis = 1)\n",
    "\n",
    "#Get name of all cities in the data frame.\n",
    "l = list(rainfall.Location.unique())\n",
    "\n",
    "#Drop all observations with NaN's.  These are values that could not be imputed using the above code.\n",
    "rainfall.dropna(subset = list(rainfall), inplace = True)\n",
    "\n",
    "#List all cities that were dropped\n",
    "for i in l:\n",
    "    if i not in rainfall.Location.unique():\n",
    "        print(i)\n",
    "        \n",
    "#'Date' and 'Location' variables not needed for prediction. \n",
    "rainfall = rainfall.drop(['Date', 'Location'], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation 2\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 2\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit as ss\n",
    "\n",
    "#Assign values to response variable, y, and explanatory variables, x.\n",
    "if 'RainTomorrow' in rainfall:\n",
    "    #Response variable is 'RainTomorrow'\n",
    "    y = rainfall['RainTomorrow'].values\n",
    "    \n",
    "    #Remove response variable from dataframe\n",
    "    del rainfall['RainTomorrow']\n",
    "    \n",
    "    #Everything else is the explanatory variables used in prediction.\n",
    "    x = rainfall.values \n",
    "    \n",
    "#Split our data into training and testing sets, 80% of data will be in the training set and 20% the testing set.\n",
    "#Data will be process this way 5 times, value can be change per user's judgement.  It is recommended that number\n",
    "#of iterations be at least 2 so that standard deviations can be computed.\n",
    "num_cv_iterations = 1\n",
    "num_instances = len(y)\n",
    "cv_object = ss(n_splits=num_cv_iterations, test_size  = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "* Task 1 Classification: Predict if it will rain the next day or not.\n",
    "* Task 2 Regression: Compute how much it rained that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Classification:  Predit if it will rain the next day or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier with 1 neighbors is: 0.81\n",
      "Time to Run: 40.97541666030884\n",
      "Accuracy of classifier with 2 neighbors is: 0.83\n",
      "Time to Run: 46.95420598983765\n",
      "Accuracy of classifier with 3 neighbors is: 0.84\n",
      "Time to Run: 50.23721623420715\n",
      "Accuracy of classifier with 4 neighbors is: 0.84\n",
      "Time to Run: 53.17704153060913\n",
      "Accuracy of classifier with 5 neighbors is: 0.85\n",
      "Time to Run: 55.60189986228943\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler as sts\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.naive_bayes import GaussianNB as gnb\n",
    "import time\n",
    "\n",
    "column_names = rainfall.columns\n",
    "weights = []\n",
    "weights_array = []\n",
    "\n",
    "scl_obj = sts()\n",
    "\n",
    "#Split the data into training and testing set 5 different ways, iterate through each way.\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(x,y)):\n",
    "    \n",
    "    #record time at the start of the iteration\n",
    "    #Standardize the explanatory variables of the training and testing sets' means to be around 0 with a standard deviation of 1.  \n",
    "    #Each value is subtracted from the mean and divided by the standard deviation of the whole dataset.\n",
    "    scl_obj.fit(x[train_indices])\n",
    "    X_train_scaled = scl_obj.transform(x[train_indices])\n",
    "    X_test_scaled = scl_obj.transform(x[test_indices])\n",
    "\n",
    "    for K in range(1, 11):\n",
    "        t0=time.time()\n",
    "        knn_clf = knn(n_neighbors=K)\n",
    "        knn_clf.fit(X_train_scaled,y[train_indices])\n",
    "        y_hat = knn_clf.predict(X_test_scaled)\n",
    "        acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "        print('Accuracy of KNN with %d neighbors is: %.2f'%(K,acc))\n",
    "        print (\"Time to Run:\", time.time()-t0)\n",
    "     \n",
    "    t0=time.time()\n",
    "    gnb_clf = gnb()\n",
    "    gnb_clf.fit(X_train_scaled,y[train_indices])\n",
    "    y_hat = gnb_clf.predict(X_test_scaled)\n",
    "    acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "    print('Accuracy of Gausian Naive Bayes: %.2f' %(acc))\n",
    "    print (\"Time to Run:\", time.time()-t0)\n",
    "    \n",
    "    #Perform Logistic Regression on training set.\n",
    "    t0=time.time()\n",
    "    lr_clf = lr(penalty='l2', C=0.05)\n",
    "    lr_clf.fit(X_train_scaled,y[train_indices])\n",
    "    y_hat = lr_clf.predict(X_test_scaled)\n",
    "    acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "    print('Accuracy of Logistic Regression %d: %.2f' %(iter_num, acc))\n",
    "    print (\"Time to Run:\", time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Regression: Compute how much it rained that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 5\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 6\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
